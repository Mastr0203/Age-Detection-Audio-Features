{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.stats import skew\n",
    "from random import randint\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the dataset you want to make the prediction on inside the folder \"Input\", then specify its name in the cell below in the field \"name_input\". The audios' folder must be inserted in the folder \"Input\", too.\n",
    "\n",
    "The output file with the predictions will be saved in the \"Predictions\" folder with the name specified below in the field \"name_output\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = \"..\\\\Input\\\\\"\n",
    "name_input = \"evaluation.csv\"\n",
    "input_file = path_input + name_input\n",
    "\n",
    "path_output = \"..\\\\Predictions\\\\\"\n",
    "name_output = \"submission\"\n",
    "output_file = path_output + name_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "The cell below is meant to import the dataset to predict, extract filepaths and sampling rates, and strip the column \"tempo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(input_file, index_col = 0) \n",
    "paths = dataset[\"path\"]\n",
    "dataset[\"tempo\"] = dataset[\"tempo\"].map(lambda x : x.strip(\"[]\")).astype(float) \n",
    "sampling_rates = dataset[\"sampling_rate\"]\n",
    "dataset.drop(columns = [\"sampling_rate\", \"path\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender One-Hot Encoding\n",
    "Categorical Features need to be encoded before making any kind of computation on them. Hence, the cell below is supposed to apply the one-hot encoding (preferred to the label one to avoid sorting) to the features \"gender\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = [\"gender\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness Reduction\n",
    "Many of the features exhibit skewed distributions, which could potentially affect the performance of the model we aim to build. To address this, we can reduce the skewness by applying a logarithmic transformation. However, before doing so, it is essential to ensure that no feature contains negative or zero values. If necessary, we translate the data to ensure all values are positive, and then apply the logarithmic function element-wise to each entry.\n",
    "\n",
    "We conducted several trials to assess the impact of the transformation on each distribution, and in cases where the transformation led to undesirable alterations, we chose not to apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_skew(data):\n",
    "\n",
    "    if np.any(data <= 0):  \n",
    "        data = data + np.abs(np.min(data)) + 1e-10  \n",
    "    \n",
    "    data = np.log(data)  \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"jitter\"] = reduce_skew(dataset[\"jitter\"])\n",
    "dataset[\"shimmer\"] = reduce_skew(dataset[\"shimmer\"])\n",
    "dataset[\"energy\"] = reduce_skew(dataset[\"energy\"])\n",
    "dataset[\"num_pauses\"] = reduce_skew(dataset[\"num_pauses\"])\n",
    "dataset[\"zcr_mean\"] = reduce_skew(dataset[\"zcr_mean\"])\n",
    "dataset[\"tempo\"] = reduce_skew(dataset[\"tempo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Features\n",
    "\n",
    "There exist several different audio features that can be extracted from a vocal message. The library *librosa* offers a bunch of different methods and techniques to extract those informations from a *wav* file.\n",
    "\n",
    "After a careful evaluation, we decided to extract these new features:\n",
    "\n",
    "1. **Duration** measures the length of the audio signal. Its extraction is meant to support all the other features that depend on it.\n",
    "\n",
    "2. **Spectral Centroid Variance**  measures the fluctuation in the \"brightness\" of a sound over time. It calculates how much the spectral centroid, which represents the perceived sharpness or brightness of an audio signal, varies across different frames. This feature is useful for capturing the dynamic texture of sound.\n",
    "\n",
    "3. **Fundamental Frequency (F0)** refers to the lowest frequency of a periodic waveform, often corresponding to the pitch of the sound. It is a key feature in speech, as it helps characterize the tone and pitch variations of a voice. The extracted features related to f0 are: mean, standard deviation, median, 95th percentile, 5th percentile.\n",
    "\n",
    "4. **Onset per Second** refers to the rate at which speech events (onsets) occur over time. It measures the number of times a significant sound changes, such as a syllable or beat, occurs in one second. In speech, it can indicate the pace of talking or the rhythm of speech. It is similar to tempo, but not exactly the same thing. Moreover, onset per second could be a good alternative to number of words, given that it can be considered as an indicator of the timestamps when the speaker pronounces a new term.\n",
    "\n",
    "5. **Mel Frequency Cepstral Coefficients (MFCCs)** are a representation of audio signals that captures the perceptually relevant features of sound. They are widely used in speech and audio processing tasks like speech recognition, speaker identification, and emotion analysis.\n",
    "\n",
    "MFCCs are derived by:\n",
    "\n",
    "- Dividing the audio signal into short frames.\n",
    "\n",
    "- Applying the Fourier Transform to convert the signal to the frequency domain.\n",
    "\n",
    "- Passing the spectrum through a filter bank that mimics the human ear's perception of pitch (mel scale).\n",
    "\n",
    "- Taking the logarithm of the filter bank energies to reflect human loudness perception.\n",
    "\n",
    "- Applying the Discrete Cosine Transform (DCT) to decorrelate the features and retain the most important coefficients.\n",
    "\n",
    "The model uses the first 13 MFCCs, that are usually suggested in speech detection tasks. They are then aggregated by computing the average.\n",
    "\n",
    "6. **Mel spectrogram**  is a visual representation of an audio signal's frequency content over time, mapped to the mel scale to reflect how humans perceive pitch.\n",
    "\n",
    "    To create a mel spectrogram:\n",
    "\n",
    "- The audio signal is divided into short frames.\n",
    "\n",
    "- A Fourier Transform is applied to compute the frequency spectrum for each frame.\n",
    "\n",
    "- The power spectrum is passed through a filter bank aligned with the mel scale, which models the human ear's sensitivity to different frequencies.\n",
    "\n",
    "- The result is a time-frequency representation where the frequencies are spaced according to the mel scale.\n",
    "\n",
    "The decision to divide the spectrogram into 64 bands and then extract only the first 20 bands is intended to focus the analysis on the frequency range where speech typically resides (we can observe from the dataset that minimum and maximum pitch are usually comprised in an interval that ranges from 140 to 4000 Hz). Mels spectrogram's coefficient are then aggregated by computing the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Durations**\n",
    "\n",
    "Here audio durations are extracted and the new feature \"silence_ratio\" is inserted in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_durations(paths, srs):\n",
    "    durations = []\n",
    "    for i, file in enumerate(paths):\n",
    "        y, sr = librosa.load(path_input + file, sr = srs[i]) #open audio file\n",
    "        durations.append(librosa.get_duration(y = y, sr = sr)) #extract the duration\n",
    "    return np.array(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"duration\"] = get_durations(paths, sampling_rates)\n",
    "dataset[\"silence_ratio\"] = dataset[\"silence_duration\"] / dataset[\"duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Audio Features**\n",
    "\n",
    "Each of the cells below is supposed to extract one of the required audio feature (Spectral Centroid Variance, Fundamental Frequency, Onset per Second, MFCCs, Mel Coefficients) for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spect_variance(paths, srs):\n",
    "    spect_variances = []\n",
    "    for i, file in enumerate(paths):\n",
    "        y, sr = librosa.load(path_input + file, sr = srs[i])\n",
    "\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y = y, sr = sr)\n",
    "        spect_variances.append(np.var(spectral_centroids))\n",
    "    return np.array(spect_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"spectral_centroid_var\"] = spect_variance(paths, sampling_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fundamental_frequency(paths, srs, min_pitch, max_pitch):\n",
    "    f0_means = []\n",
    "    f0_stds = []\n",
    "    f0_medians = []\n",
    "    f0_05_perc = []\n",
    "    f0_95_perc = []\n",
    "\n",
    "    for i, file in enumerate(paths):\n",
    "        y, sr = librosa.load(path_input + file, sr = srs[i])\n",
    "\n",
    "        f0 = librosa.yin(y, fmin=min_pitch[i], fmax=max_pitch[i], sr=sr)\n",
    "        f0_means.append(np.nanmean(f0))\n",
    "        f0_stds.append(np.nanstd(f0))\n",
    "        f0_medians.append(np.nanmedian(f0))\n",
    "        f0_05_perc.append(np.nanpercentile(f0, 5))\n",
    "        f0_95_perc.append(np.nanpercentile(f0, 95))\n",
    "    return np.array(f0_means), np.array(f0_stds), np.array(f0_medians), np.array(f0_05_perc), np.array(f0_95_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_means, f0_stds, f0_medians, f0_05_perc, f0_95_perc = extract_fundamental_frequency(paths, sampling_rates, dataset[\"min_pitch\"], dataset[\"max_pitch\"])\n",
    "dataset[\"f0_mean\"] = f0_means\n",
    "dataset[\"f0_std\"] = f0_stds\n",
    "dataset[\"f0_median\"] = f0_medians\n",
    "dataset[\"f0_05_perc\"] = f0_05_perc\n",
    "dataset[\"f0_95_perc\"] = f0_95_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_onsets_per_seconds(paths, srs, durations):\n",
    "    onsets_per_second = []\n",
    "    for i, file in enumerate(paths):\n",
    "        y, sr = librosa.load(path_input + file, sr = srs[i])\n",
    "\n",
    "        onset_frames = librosa.onset.onset_detect(y=y, sr = sr, hop_length=512, backtrack=True)\n",
    "        onsets_per_second.append(len(onset_frames) / durations[i])\n",
    "    return np.array(onsets_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"onsets_per_second\"] = extract_onsets_per_seconds(paths, sampling_rates, dataset[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(paths, n_mfccs, srs):\n",
    "    mfcc_means = []\n",
    "    for i, file in enumerate(paths):\n",
    "        y, sr = librosa.load(path_input + file, sr = srs[i])\n",
    "        mfccs = librosa.feature.mfcc(y = y, sr = sr, n_mfcc = n_mfccs)\n",
    "        mfcc_mean = np.mean(mfccs, axis = 1)  # Mean MFCCs\n",
    "        mfcc_means.append(mfcc_mean)\n",
    "    return pd.DataFrame(mfcc_means, columns = [f\"Mfcc_{i+1}_mean\" for i in range(n_mfccs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_means = extract_mfcc(paths, n_mfccs = 13, srs = sampling_rates)\n",
    "dataset = pd.concat((dataset, mfccs_means), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is not meant to extract a feature but to normalize the lengths of audio vocals by randomly extractin *target_duration* secods from the audio. It will be used to extract mel's coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(y, sr, target_duration = 7):\n",
    "        \"\"\"Normalizza la lunghezza del file audio a target_duration.\"\"\"\n",
    "        target_length = int(target_duration * sr)\n",
    "        if len(y) > target_length: \n",
    "            start_frame = randint(0, len(y) - target_length)\n",
    "            end_frame = start_frame + target_length\n",
    "            return y[start_frame : end_frame]\n",
    "        elif len(y) < target_length: \n",
    "            padding = target_length - len(y)\n",
    "            return np.pad(y, (0, padding), mode='constant')\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel(paths, n_mels, srs):\n",
    "    mels_means = []\n",
    "\n",
    "    for i, file in enumerate(paths):\n",
    "        y, sr = librosa.load(path_input + file, sr = srs[i])\n",
    "        y_normalized = normalize_audio(y, sr, target_duration = 7)\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y_normalized, sr=sr, n_mels=n_mels)\n",
    "        log_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        mels_means.append(np.mean(log_spectrogram, axis=1))\n",
    "\n",
    "    return pd.DataFrame(mels_means, columns = [f\"Mel_{i+1}_mean\" for i in range(n_mels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "mels_mean = extract_mel(paths, n_mels = 64, srs = sampling_rates)\n",
    "dataset = pd.concat((dataset, mels_mean.iloc[:, :20]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns\n",
    "The cell below is meant to drop all the columns corresponding to the feature we decided to discard. It follows a brief list of the reasons that lead us to discard them:\n",
    "\n",
    "1. *Ethnicity*: Ages are not uniformly distributed across the ethnicities, so making a decision on this attribute leads to unfairly exploit the structure of the training set. To further confirm this thesis, we did also compute the cosine similarity of the centroids between different couples of ethnicities that should not have nothing in common, and this turned to be really close to 1 in basically any of the cases. \n",
    "\n",
    "2. *Min Pitch and Max Pitch*: Distributions for males and females and for distinct ages are so similar that could be considered identical: differences are so small that cannot be perceived. Hence, it makes no sense to maintain those attributes in the dataset given that they do not contribute to distinguish speakers.\n",
    "\n",
    "3. *Num Words and Num Characters*: A manual inspection of the recordings confirm this thesis: in most of the audios, speakers asks to a third person to tell Stella to bring a list of things from a store. There are some other short messages like \"I'm from Nigeria\" or \"I'm hungry and thirsty\" scattered in the dataset. Moreover, the inspection revealed that many of the audio present num_words and num_characters equal to 0, even though someone actually speaks. We do not have the certainty but, given that most of 0-characters audio are not in English, we can suppose that something went wrong with the transcription.\n",
    "\n",
    "4. *Silence Duration*: it has been normalized by the duration before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns = [\"ethnicity\"], inplace = True)\n",
    "dataset.drop(columns = [\"min_pitch\"], inplace = True)\n",
    "dataset.drop(columns = [\"max_pitch\"], inplace = True)\n",
    "dataset.drop(columns = [\"num_words\"], inplace = True)\n",
    "dataset.drop(columns = [\"num_characters\"], inplace = True)\n",
    "dataset.drop(columns = [\"silence_duration\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment these lines if you are using the development dataset.\n",
    "\n",
    "#dataset.drop(columns = [\"age\"], inplace = True)\n",
    "#dataset.to_csv(\"..\\\\Datasets\\\\Preprocessed Data\\\\Training\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "We chose the Ridge regression model for our task of predicting ages from vocal features due to its ability to handle datasets with a large number of features effectively. Ridge regression adds an ð¿2 regularization penalty to the loss function, which helps prevent overfitting by shrinking the coefficients of less important features. This was particularly important in our case, as we were working with a dataset containing many features, some of which might be correlated or not strongly predictive of age. By incorporating regularization, Ridge regression ensures a more robust and generalizable model that balances predictive accuracy with feature stability.\n",
    "\n",
    "Additionally, we implemented Polynomial Features of degree 2 at the beginning of the pipeline to capture potential nonlinear relationships between the features and the target variable. By combining polynomial feature engineering with regularization, Ridge regression ensures a more robust and generalizable model that balances predictive accuracy with feature stability.\n",
    "Finally, to ensure all features were on a comparable scale, we applied a Standard Scaler after generating the polynomial features.\n",
    "\n",
    "The cells below defines the model and fit it with the training set stored in the appropriate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"..\\\\Datasets\\\\Preprocessed Data\\\\Training\")\n",
    "age = pd.read_csv(\"..\\\\Datasets\\\\Original Data\\\\development.csv\", usecols = [\"age\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"poly\" , PolynomialFeatures(2)),\n",
    "    (\"scaler\" , StandardScaler()),\n",
    "    (\"ridge\" , Ridge(alpha = 178))\n",
    "])\n",
    "\n",
    "X_train, y_train = df_train, age.values\n",
    "pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the cell below is meant to make the predictions on the given input, saving the resulting file in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(df_train).flatten()\n",
    "\n",
    "predictions = pd.DataFrame({\"Id\" : [i for i in range(y_pred.size)], \"Predicted\" : y_pred})\n",
    "predictions.to_csv(output_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
